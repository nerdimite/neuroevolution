{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1d8f7c77908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    '''The brain of the agent'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(4, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 2))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_population(pop_size=2):\n",
    "    '''Randomly initialize a bunch of agents'''\n",
    "    population = [Agent() for _ in range(pop_size)]\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, episodes=15, max_episode_length=250):\n",
    "    '''Run an agent for a given number episodes and get the rewards'''\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    agent.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        # Modify the maximum steps that can be taken in a single episode\n",
    "        env._max_episode_steps = max_episode_length\n",
    "        \n",
    "        episodic_reward = 0\n",
    "        # Start episode\n",
    "        for step in range(max_episode_length):\n",
    "            input_obs = torch.Tensor(observation).unsqueeze(0)\n",
    "            observation, reward, done, info = env.step(agent(input_obs).argmax(dim=1).item())\n",
    "            \n",
    "            episodic_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(episodic_reward)\n",
    "                \n",
    "    return np.array(total_rewards).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(population, episodes=15, max_episode_length=250):\n",
    "    '''Evaluate the population'''\n",
    "    pop_fitness = []\n",
    "    \n",
    "    for agent in population:\n",
    "        pop_fitness.append(evaluate_agent(agent, episodes, max_episode_length))\n",
    "        \n",
    "    return pop_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(parent_agent, mutation_power=0.02):\n",
    "    '''Creates a mutated copy of the parent agent by adding a weighted gaussian noise to the params'''\n",
    "    child_agent = copy.deepcopy(parent_agent)\n",
    "    \n",
    "    for param in child_agent.parameters():\n",
    "        param.data = param.data + (torch.randn(param.shape) * mutation_power)\n",
    "        \n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repopulate(top_agents, pop_size, mutation_power):\n",
    "    '''Repopulate the population from the top agents by mutation'''\n",
    "    new_population = []\n",
    "    \n",
    "    n = 0\n",
    "    while(n < pop_size):\n",
    "        for parent in top_agents:\n",
    "            child = mutate(parent, mutation_power)\n",
    "            new_population.append(child)\n",
    "            n += 1\n",
    "            \n",
    "    return new_population[:pop_size - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_AGENT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(generations=10, max_time=60, \n",
    "           pop_size=100, \n",
    "           topK=20, \n",
    "           episodes=15, \n",
    "           max_episode_length=250, \n",
    "           mutation_power=0.02):\n",
    "    '''Start the process of evolution'''\n",
    "    \n",
    "    global TRAINED_AGENT\n",
    "    \n",
    "    population = initialize_population(pop_size)\n",
    "    global_best = {}\n",
    "    \n",
    "    t1 = time()\n",
    "#     g = 0 # uncomment when using max_time for training instead of generations\n",
    "    for g in range(generations):\n",
    "#     while ((time() - t1) <= max_time): # uncomment when using max_time for training instead of generations\n",
    "        \n",
    "        # Evaluate the population\n",
    "        pop_fitness = evaluate_population(population, episodes, max_episode_length)\n",
    "        mean_pop_reward = np.array(pop_fitness).mean()\n",
    "        \n",
    "        # Rank the agents in descending order\n",
    "        topK_idx = np.argsort(pop_fitness)[::-1][:topK]\n",
    "        topK_agents = [population[i] for i in topK_idx]\n",
    "        \n",
    "        # Get Best Agent\n",
    "        best_agent = population[topK_idx[0]]\n",
    "        best_reward = pop_fitness[topK_idx[0]]\n",
    "        \n",
    "        # Check with global best\n",
    "        if g == 0:\n",
    "            global_best['reward'] = best_reward\n",
    "            global_best['agent'] = best_agent\n",
    "        else:\n",
    "            if best_reward >= global_best['reward']:\n",
    "                global_best['reward'] = best_reward\n",
    "                global_best['agent'] = best_agent\n",
    "                \n",
    "        print('Generation', g)\n",
    "        print('Mean Reward of Population', mean_pop_reward)\n",
    "        print('Best Agent Reward (mean)', best_reward)\n",
    "        print('Global Best Reward (mean)', global_best['reward'], '\\n')\n",
    "        \n",
    "        # Mutate and Repopulate\n",
    "        new_population = repopulate(topK_agents, pop_size, mutation_power)\n",
    "        # take the best agent of generation forward without cloning as well\n",
    "        new_population.append(best_agent)\n",
    "        \n",
    "        population = new_population\n",
    "        \n",
    "        TRAINED_AGENT = global_best\n",
    "        \n",
    "#         g += 1 # uncomment when using max_time for training instead of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0\n",
      "Mean Reward of Population 15.926666666666668\n",
      "Best Agent Reward (mean) 85.4\n",
      "Global Best Reward (mean) 85.4 \n",
      "\n",
      "Generation 1\n",
      "Mean Reward of Population 31.750000000000007\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 2\n",
      "Mean Reward of Population 38.81333333333334\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 3\n",
      "Mean Reward of Population 76.23666666666668\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 4\n",
      "Mean Reward of Population 102.73333333333335\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 5\n",
      "Mean Reward of Population 134.02333333333334\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 6\n",
      "Mean Reward of Population 141.35666666666663\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 7\n",
      "Mean Reward of Population 163.77333333333334\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 8\n",
      "Mean Reward of Population 181.95999999999998\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 9\n",
      "Mean Reward of Population 169.31333333333333\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 10\n",
      "Mean Reward of Population 182.92666666666668\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 11\n",
      "Mean Reward of Population 177.22\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 12\n",
      "Mean Reward of Population 166.91\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 13\n",
      "Mean Reward of Population 166.7866666666667\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 14\n",
      "Mean Reward of Population 150.94666666666666\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 15\n",
      "Mean Reward of Population 148.99\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 16\n",
      "Mean Reward of Population 165.35\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 17\n",
      "Mean Reward of Population 158.28333333333336\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 18\n",
      "Mean Reward of Population 178.41333333333336\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n",
      "Generation 19\n",
      "Mean Reward of Population 170.71333333333334\n",
      "Best Agent Reward (mean) 200.0\n",
      "Global Best Reward (mean) 200.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evolve(generations=20,\n",
    "       pop_size=20, \n",
    "       topK=10, \n",
    "       episodes=15, \n",
    "       max_episode_length=200, \n",
    "       mutation_power=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(agent, episodes=5, max_episode_length=200, render=False):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    agent.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        observation = env.reset()\n",
    "        env._max_episode_steps = max_episode_length\n",
    "        \n",
    "        episodic_reward = 0\n",
    "        \n",
    "        for step in range(max_episode_length):\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            input_obs = torch.Tensor(observation).unsqueeze(0)\n",
    "            observation, reward, done, info = env.step(agent(input_obs).argmax(dim=1).item())\n",
    "            \n",
    "            episodic_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        total_rewards.append(episodic_reward)\n",
    "    \n",
    "    env.close()\n",
    "    print('Mean Rewards across all episodes', np.array(total_rewards).mean())\n",
    "    print('Best Reward in any single episode', max(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rewards across all episodes 200.0\n",
      "Best Reward in any single episode 200.0\n"
     ]
    }
   ],
   "source": [
    "play_agent(TRAINED_AGENT['agent'], episodes=100, max_episode_length=200, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
